{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RBM_Github.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uc6LqbuMLMdz"
      },
      "source": [
        "# Restricted Boltzmann Machines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4hl6QtzM2zJ"
      },
      "source": [
        "# Part 1 - Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsmJ8nDAKjfc"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.utils.data\n",
        "import torch.optim as optim\n",
        "import torch.nn.parallel\n",
        "import torch.nn as nn"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8HkY7iVM9hg"
      },
      "source": [
        "Importing the Training and Test Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "y9IBRPV1HXrB",
        "outputId": "71b0b4e3-a6ab-46a4-bccb-6888bcb38c7c"
      },
      "source": [
        "test_set = pd.read_csv('/test_set.csv')\r\n",
        "train_set = pd.read_csv('/training_set.csv')\r\n",
        "\r\n",
        "train_set\r\n",
        "#column 0 = user ID, column 1 = movie ID, column 2 = rating, column 3 = time stamp"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>User</th>\n",
              "      <th>Movie</th>\n",
              "      <th>Rating</th>\n",
              "      <th>Timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>661</td>\n",
              "      <td>3</td>\n",
              "      <td>978302109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>914</td>\n",
              "      <td>3</td>\n",
              "      <td>978301968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3408</td>\n",
              "      <td>4</td>\n",
              "      <td>978300275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>2355</td>\n",
              "      <td>5</td>\n",
              "      <td>978824291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1287</td>\n",
              "      <td>5</td>\n",
              "      <td>978302039</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>750116</th>\n",
              "      <td>6040</td>\n",
              "      <td>1091</td>\n",
              "      <td>1</td>\n",
              "      <td>956716541</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>750117</th>\n",
              "      <td>6040</td>\n",
              "      <td>1094</td>\n",
              "      <td>5</td>\n",
              "      <td>956704887</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>750118</th>\n",
              "      <td>6040</td>\n",
              "      <td>562</td>\n",
              "      <td>5</td>\n",
              "      <td>956704746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>750119</th>\n",
              "      <td>6040</td>\n",
              "      <td>1096</td>\n",
              "      <td>4</td>\n",
              "      <td>956715648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>750120</th>\n",
              "      <td>6040</td>\n",
              "      <td>1097</td>\n",
              "      <td>4</td>\n",
              "      <td>956715569</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>750121 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        User  Movie  Rating  Timestamp\n",
              "0          1    661       3  978302109\n",
              "1          1    914       3  978301968\n",
              "2          1   3408       4  978300275\n",
              "3          1   2355       5  978824291\n",
              "4          1   1287       5  978302039\n",
              "...      ...    ...     ...        ...\n",
              "750116  6040   1091       1  956716541\n",
              "750117  6040   1094       5  956704887\n",
              "750118  6040    562       5  956704746\n",
              "750119  6040   1096       4  956715648\n",
              "750120  6040   1097       4  956715569\n",
              "\n",
              "[750121 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "72rBFv-gHm3o",
        "outputId": "7c4b5fc1-68e9-442f-c6e2-4c77c40a2437"
      },
      "source": [
        "test_set\r\n",
        "#so 250088 + 750121 = 1 million total ratings "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>User</th>\n",
              "      <th>Movie</th>\n",
              "      <th>Rating</th>\n",
              "      <th>Timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1193</td>\n",
              "      <td>5</td>\n",
              "      <td>978300760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1197</td>\n",
              "      <td>3</td>\n",
              "      <td>978302268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>2804</td>\n",
              "      <td>5</td>\n",
              "      <td>978300719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>595</td>\n",
              "      <td>5</td>\n",
              "      <td>978824268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>938</td>\n",
              "      <td>4</td>\n",
              "      <td>978301752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>250083</th>\n",
              "      <td>6040</td>\n",
              "      <td>3735</td>\n",
              "      <td>4</td>\n",
              "      <td>960971654</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>250084</th>\n",
              "      <td>6040</td>\n",
              "      <td>2791</td>\n",
              "      <td>4</td>\n",
              "      <td>956715569</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>250085</th>\n",
              "      <td>6040</td>\n",
              "      <td>527</td>\n",
              "      <td>5</td>\n",
              "      <td>956704219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>250086</th>\n",
              "      <td>6040</td>\n",
              "      <td>2003</td>\n",
              "      <td>1</td>\n",
              "      <td>956716294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>250087</th>\n",
              "      <td>6040</td>\n",
              "      <td>535</td>\n",
              "      <td>4</td>\n",
              "      <td>964828734</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>250088 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        User  Movie  Rating  Timestamp\n",
              "0          1   1193       5  978300760\n",
              "1          1   1197       3  978302268\n",
              "2          1   2804       5  978300719\n",
              "3          1    595       5  978824268\n",
              "4          1    938       4  978301752\n",
              "...      ...    ...     ...        ...\n",
              "250083  6040   3735       4  960971654\n",
              "250084  6040   2791       4  956715569\n",
              "250085  6040    527       5  956704219\n",
              "250086  6040   2003       1  956716294\n",
              "250087  6040    535       4  964828734\n",
              "\n",
              "[250088 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGKVWAWJJEsu"
      },
      "source": [
        "Converting our Dataframes into Numpy Arrays"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQa4029zpVCT",
        "outputId": "8609efd6-c4ba-4174-cd96-72e165b7057e"
      },
      "source": [
        "train_set = np.array(train_set, dtype = 'int') #specified the data type for our data, which is all integers anyway since its ratings and IDs\r\n",
        "test_set = np.array(test_set, dtype = 'int') #specified the data type for our data, which is all integers anyway since its ratings and IDs\r\n",
        "\r\n",
        "train_set"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[        1,       661,         3, 978302109],\n",
              "       [        1,       914,         3, 978301968],\n",
              "       [        1,      3408,         4, 978300275],\n",
              "       ...,\n",
              "       [     6040,       562,         5, 956704746],\n",
              "       [     6040,      1096,         4, 956715648],\n",
              "       [     6040,      1097,         4, 956715569]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zM_SKin0RDfI"
      },
      "source": [
        "Getting the total number of users and movies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IM1hsOkFT6ue",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0be4423-2d11-4803-ebac-21675d122c0d"
      },
      "source": [
        "num_users = int(max(max(test_set[:,0]),max(train_set[:,0]))) #getting the maximum user ID in the training and test set from all the rows, and column 0 which is the user ID\n",
        "num_movies = int(max(max(test_set[:,1]),max(train_set[:,1]))) #getting the maximum movie ID in the training and test set from all the rows, and column 1 which is the movie ID\n",
        "\n",
        "print(num_users) #so max userID is 6040 and max movieID is 3952\n",
        "print(num_movies)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6040\n",
            "3952\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujeJIQnLVrDL"
      },
      "source": [
        "Converting Datasets into Array with 6040 rows (users) and 3952 columns (ratings)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSS0fFuGUwwx"
      },
      "source": [
        "#Here we're going to create a list of lists. Basically we'll have 6040 lists (# of users), and each list will have 3952 movies (with movies they haven't rated equal to 0). We do this with a numpy array to make it easier to work with pytorch afterwards\n",
        "def ratingslist(data):\n",
        "  ratings_list = []\n",
        "  for user_id in range(1,num_users+1):\n",
        "    #starting from 1 since first user_ID is 1 and ending on 6040+1 since python doesn't include the upper bound\n",
        "    movie_id = data[:,1][data[:,0] == user_id] #so here we're taking all the rows from column 1, which is the movie ids from the training/test set. We're also making it so that it only takes the movies watched by a specific user id. To do this, we basically took column 0 and said it must be equal to user_id from the training/test set\n",
        "    rating_id = data[:,2][data[:,0] == user_id] #so here we're taking all the rows from column 2, which is the ratings from the training/test sets. We're also making it so that it only takes the ratings by a specific user id. To do this, we basically took column 0 and said it must be equal to user_id from the training/test set\n",
        "    rating = np.zeros(num_movies) #creating a list of 3952 movies initialized with all zeros, so we can then populate it with ratings from users afterwards, and any movies they didn't rate will be given a value of 0\n",
        "    rating[movie_id - 1] = rating_id #did -1 because python index starts at 0, but movie_id starts with 1\n",
        "    ratings_list.append(list(rating)) #now adding the ratings of user_id to the ratings list\n",
        "  return ratings_list\n",
        "\n",
        "train_set = ratingslist(train_set) #now we're using our function to convert our training set into a list of lists\n",
        "test_set = ratingslist(test_set) #same above but for test set"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jE_oMbiyPMJd",
        "outputId": "cd443ae6-5349-4b67-c0a5-c0c5ba39807c"
      },
      "source": [
        "print(\"Number of Rows (users) =\",len(test_set)) #so it's 6040 users\r\n",
        "print(\"Number of Cols (ratings) =\", len(test_set[0])) #and the number of columns is 3952 ratings (1 for each movie, 0 if unwatched movie)\r\n",
        "print(\"User 1, Movie 661, Rating =\",train_set[0][660]) #we can see for user 1, movie 660, the rating given was 3 which reflects the first row in the training set when first imported\r\n",
        "print(\"User 6040, Movie 535, Rating =\",test_set[6039][534]) #we can see for user 6040, movie 535, the rating given was 4 which reflects the last row in the test set when first imported"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Rows (users) = 6040\n",
            "Number of Cols (ratings) = 3952\n",
            "User 1, Movie 661, Rating = 3.0\n",
            "User 6040, Movie 535, Rating = 4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUhzNLSHbghD"
      },
      "source": [
        "Converting Array into Torch Tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VQGKGC8ahNR"
      },
      "source": [
        "#so we could create a neural network with numpy arrays, but pytorch arrays are far more efficient, which is why we're using it\n",
        "train_set = torch.FloatTensor(train_set) #FloatTensor expects a list of lists\n",
        "test_set = torch.FloatTensor(test_set) #FloatTensor expects a list of lists"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2JzFFzCSBj9",
        "outputId": "59d996cd-b80a-4ad4-ae69-117f10b8d4ca"
      },
      "source": [
        "#Converting to Torch Tensor kept the shapes the same\r\n",
        "print(\"Number of Rows (users) =\",len(test_set)) #so it's 6040 users\r\n",
        "print(\"Number of Cols (ratings) =\", len(test_set[0])) #and the number of columns is 3952 ratings (1 for each movie, 0 if unwatched movie)\r\n",
        "print(\"User 1, Movie 661, Rating =\",train_set[0][660]) #we can see for user 1, movie 660, the rating given was 3 which reflects the first row in the training set when first imported\r\n",
        "print(\"User 6040, Movie 535, Rating =\",test_set[6039][534]) #we can see for user 6040, movie 535, the rating given was 4 which reflects the last row in the test set when first imported"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Rows (users) = 6040\n",
            "Number of Cols (ratings) = 3952\n",
            "User 1, Movie 661, Rating = tensor(3.)\n",
            "User 6040, Movie 535, Rating = tensor(4.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCCCH_TK_sJF"
      },
      "source": [
        "Converting the ratings into Binary Ratings 1 (Liked) or 0 (Disliked)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65L5YNo8cU6I"
      },
      "source": [
        "train_set[train_set == 0] = -1 #replacing all movies that were rated as 0 (not rated) is now -1\n",
        "train_set[train_set == 1] = 0 #replacing all 1 or 2 rated movies as disliked\n",
        "train_set[train_set == 2] = 0 #replacing all 1 or 2 rated movies as disliked\n",
        "#can't just do <= 2 because then the -1 we converted for 0 ratings would be converted back to 0\n",
        "train_set[train_set > 2] = 1 #replacing all 3, 4, or 5 rated movies as liked\n",
        "\n",
        "test_set[test_set == 0] = -1 #replacing all movies that were rated as 0 (not rated) is now 0\n",
        "test_set[test_set == 1] = 0 #replacing all 1 or 2 rated movies as disliked\n",
        "test_set[test_set == 2] = 0 #replacing all 1 or 2 rated movies as disliked \n",
        "#can't just do <= 2 because then the -1 we converted for 0 ratings would be converted back to 0\n",
        "test_set[test_set > 2] = 1 #replacing all 3, 4, or 5 rated movies as liked"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TN5vZbK3WKrE",
        "outputId": "49fc58d9-66c8-4298-ea02-b23bef85bad0"
      },
      "source": [
        "print(\"User 117, Movie 343, Rating =\",train_set[116][342]) #we can see for user 117, movie 343, no rating was given\r\n",
        "print(\"User 1, Movie 661, Rating =\",train_set[0][660]) #we can see for user 1, movie 660, the rating given was 3 which means it was considered a liked film\r\n",
        "print(\"User 6040, Movie 2003, Rating =\",test_set[6039][2002]) #we can see for user 6040, movie 2002, the rating given was 0 which means it was considered a disliked film"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "User 117, Movie 343, Rating = tensor(-1.)\n",
            "User 1, Movie 661, Rating = tensor(1.)\n",
            "User 6040, Movie 2003, Rating = tensor(0.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "try3evq0BnzW"
      },
      "source": [
        "# Part 2 - RBM Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFykX2GXBf0s"
      },
      "source": [
        "Creating the Architecture of the Neural Network - Restricted Boltzmann Machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "botOArEyBGTW"
      },
      "source": [
        "class RBM():\n",
        "  #num_vis is number of visible nodes, num_hid is number of hidden nodes\n",
        "  def __init__(self, num_vis, num_hid):\n",
        "    self.W = torch.randn(num_hid, num_vis) #W is the weights of the parameters of the probabilities of the visible nodes given the hidden nodes (prob_vgh). Since we're using PyTorch, this is gonna be a torched tensor\n",
        "    #So above, we are randomly initializing the weights of a tensor of size num_hid and num_vis based on normal distribution\n",
        "    #so this initialized the probability, P, of the visibile nodes given the hidden nodes\n",
        "    self.a = torch.randn(1, num_hid) #Now we are initializing the bias for the probability of the Hidden Nodes given the Visible Nodes, prob_hgv.\n",
        "    #So above, the bias needs to be 2 dimensions. So the first dimension corresponds to the batch = 1, whereas the second corresponds to the number of hidden nodes. This is because PyTorch cannot accept a single vector of one dimension as an argument, so we create a fake dimension with the batch size\n",
        "    self.b = torch.randn(1, num_vis) #Now we are initializing the bias for the probabilitiy of the Visible Nodes given the Hidden Nodes, prob_vgh.\n",
        "\n",
        "  #so the function below returns a sample of different hidden nodes in our RBM according to a certain probability (that will be computed in this same function, prob_hgv). For each of these hidden nodes, this probability is prob_hgv, which is equal to the sigmoid activation function for the hidden nodes\n",
        "  #x will correspond to the visible neurons, V, in the probability, prob_hgv. The sigmoid activation function is applied to wx, which is the product of w (vector of weights) times x (vector of visible neurons) plus the bias, a (bias for probability of hidden nodes given visible, prob_hgv)\n",
        "  def sample_h(self, x):\n",
        "    wx = torch.mm(x, self.W.t()) #.mm is used to create a product of two torch tensors(matrix 1 times matrix 2). self.W.t() is the weights, but transposed, so we can properly multiply the matrices\n",
        "    activation = wx + self.a.expand_as(wx) #recall that each input vector will not be treat individually, but inside batches. So when we apply the bias, a, we want to make sure its applied to each line of each dimension (so batch size = 1 and num_hid). So the function expand_as makes sure our matrix expands to fit the target matrix, wx, so we can still do the add operation (and therefore the bias applies to every line of each dimension)\n",
        "    prob_hgv = torch.sigmoid(activation) #This is basically the probabilty that the Hidden Node Activates given the Visible Node (input) i.e if someone likes a drama film, the visible node for said drama film would activate, thus making a hidden node related to drama genre very likely to activate as well\n",
        "    return prob_hgv, torch.bernoulli(prob_hgv) #so say we have 100 hidden nodes. prob_hgv will return the probability of the i-th hidden node activating given the input. Then using Bernoulli sampling, we take the probabilty of hidden node i (say 70%), and if a random number between 0 and 1 fall below probability_i (<70%), the hidden node activates, and if it's above probability_i (>70%), the hidden node doesn't activate. And we do that for each of our 100 hidden nodes, which is how Bernoulli Sampling works. This results in a vector of 0s and 1s, where 0s correspond to hidden nodes activated by this Bernoulli sampling, and 1 vise versa. This Bernoulli function returns this sampling of hidden nodes to activate.\n",
        "\n",
        "  #so the function below returns a sample of different visible nodes in our RBM according to a certain probability (that will be computed in this same function, prob_vgh). For each of these visible nodes, this probability is prob_vgh, which is equal to the sigmoid activation function for the visible nodes\n",
        "  #y will correspond to the hidden neurons, H, in the probability, prob_vgh. The sigmoid activation function is applied to wy, which is the product of w (vector of weights) times y (vector of hidden neurons) plus the bias, b (bias for probability of visible nodes given hidden, prob_vgh)\n",
        "  def sample_v(self, y):\n",
        "    wy = torch.mm(y, self.W) #.mm is used to create a product of two torch tensors(matrix 1 times matrix 2). self.W is the weights for prob_vgh, but not transposed since the default shape corresponds to y for matrix multiplication\n",
        "    activation = wy + self.b.expand_as(wy) #recall that each input vector will not be treat individually, but inside batches. So when we apply the bias, b, we want to make sure its applied to each line of each dimension (so batch size = 1 and num_vis). So the function expand_as makes sure our matrix expands to fit the target matrix, wx, so we can still do the add operation (and therefore the bias applies to every line of each dimension)\n",
        "    prob_vgh = torch.sigmoid(activation) #This is basically the probabilty that the Visible Node Activates given the Hidden Node (input) i.e if someone likes a dramas, the hidden node for said drama genres would activate, thus making a visible node providing a drama film very likely to activate as well\n",
        "    return prob_vgh, torch.bernoulli(prob_vgh) #so say we have 6040 visible nodes (num_movies). prob_vgh will return the probability of the i-th visible node activating given the hidden node. Then using Bernoulli sampling, we take the probabilty of visible node i (say 70%), and if a random number between 0 and 1 fall below probability_i (<70%), the visible node activates, and if it's above probability_i (>70%), the hidden node doesn't activate. And we do that for each of our 6040 visible nodes, which is how Bernoulli Sampling works. This results in a vector of 0s and 1s, where 0s correspond to visible nodes activated by this Bernoulli sampling, and 1 vise versa. This Bernoulli function returns this sampling of visible nodes to activate.\n",
        "\n",
        "  #v0 is the input vector of all the ratings by one user (visible nodes), vk is the visible nodes after k-samplings (k round trips from visible node to hidden node back to visible node) in Contrastive Divergence, ph0 is the vector of probabilities that at the first iteration the hidden nodes activating given the values of v0 (input vector), phk is the probabilty of the hidden nodes activating after K sampling given the value of the visible nodes after k-samplings (vk)\n",
        "  #The function below will compute the K-Contrastive Divergence algorithm by updating weights and the biases, as we discussed in the theory\n",
        "  #http://cms.dm.uba.ar/academico/materias/1ercuat2018/probabilidades_y_estadistica_C/5a89b5075af5cbef5becaf419457cdd77cc9.pdf \n",
        "  #see page 15 for the algorithm\n",
        "  def train(self, v0, vk, ph0, phk):\n",
        "    self.W += (torch.mm(v0.t(), ph0) - torch.mm(vk.t(), phk)).t() #This equation is from the RBM paper. This is the update the weights. The final .t() is to transpose the Weight matrix\n",
        "    self.b += torch.sum((v0 - vk), 0) #v0 - vk is the actual equation, 0 is just to keep it in a 2 dimensional format. Matrix size of b should be 100 x 3942\n",
        "    self.a += torch.sum((ph0 - phk), 0) #same as above. Matrix size of should be 100 x 100\n",
        "    "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PnsdmnZilj7"
      },
      "source": [
        "Calling our class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDU9abB_ry23",
        "outputId": "310025f1-ad62-41ff-8faa-62bc90d30edd"
      },
      "source": [
        "num_vis = len(train_set[0]) #number of visibile nodes is the number of movies\r\n",
        "num_vis"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3952"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03UesH0GsBnc"
      },
      "source": [
        "num_hid = 100 #just picking 100 to start with since the optimal number of hidden nodes is hard to predict, but can be tuned later \r\n",
        "batch_size = 100 #just picking 100 to start with, but can be tuned later"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oISUj8HNYbs7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1d3ca3c-c24b-4386-a9d6-ed0d06712350"
      },
      "source": [
        "rbm = RBM(num_vis, num_hid) #Calling the class we made above\n",
        "rbm"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.RBM at 0x7ffb5616f358>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BX67S5zGjDXw"
      },
      "source": [
        "# Part 3 -Training the RMB Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ua45Yzwi2Mw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d9a1008-2e61-49ec-c0ec-0ae5a57ac1ec"
      },
      "source": [
        "import time #Gonna try to keep track of how long it takes to train our model\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "num_epoch = 20\n",
        "\n",
        "#http://cms.dm.uba.ar/academico/materias/1ercuat2018/probabilidades_y_estadistica_C/5a89b5075af5cbef5becaf419457cdd77cc9.pdf\n",
        "for epoch in range(1, num_epoch + 1):\n",
        "  training_loss = 0 #initializing the training loss\n",
        "  s = 0. #s is the counter we will use to normalize the training loss by dividing train loss by s\n",
        "  #For below, since the batch size is 100, means that the first user is 0, and the last user will be number of users (6040) minus the batch size (100), which is 5940. The third argument is the step size, which is the batch size\n",
        "  for id_user in range(0, num_users - batch_size, batch_size):\n",
        "    vk = train_set[id_user:id_user+batch_size] #this gives us our batch of 100 users, since it's say id_user=0, then up to id_user=0 + batch size = 100\n",
        "    v0 = train_set[id_user:id_user+batch_size] #original ratings of movies by the 100 users in the batch size\n",
        "    ph0,_ = rbm.sample_h(v0) #recall ph0 is the probability that the hidden nodes activate (equal 1) given the visible nodes at the very beginning. The ,_ means we only want to return the first element from our sample_h function, which is prob_hgv\n",
        "    for k in range(15):\n",
        "      #So here, we want to continuously update the visible node. To do this, we take the bernoulli sampling element from the sample_h function and use it to get the samples of hidden nodes, which will lead us to get the next visible node values, vk\n",
        "      _,hk = rbm.sample_h(vk) #hk is the hidden nodes obtained at the k-th step of Contrastive Divergence. The _, means the function only returns the second element, which is the bernoulli sampling. Then we use vk (which is equal to v0 initially) in the iterations to get the probability of hidden nodes given visibile, Ph_given_v\n",
        "      _,vk = rbm.sample_v(hk) #so here we are updating our vk based on the hk we found above, basically getting bernoulli sample of prob_vgh\n",
        "      vk[v0 < 0] = v0[v0 < 0] #This is to just make sure that the movies that weren't rated (which equals -1) in the original visible nodes, v0, stay as unrated in the later visible nodes, vk. That way, we dont do training on unrated movies\n",
        "    phk,_ = rbm.sample_h(vk) #Just getting the final probability of hidden nodes given the final visible nodes after k samples\n",
        "\n",
        "    rbm.train(v0, vk, ph0, phk) #training it\n",
        "    training_loss += torch.mean(torch.abs(v0[v0 >= 0] - vk[v0 >= 0])) #the loss is just the mean difference between the original visible node ratings and the final visible node ratings after k-iterations/samples\n",
        "    s += 1. #this is just incrementing the counter\n",
        "  print('epoch: ' + str(epoch) + ' loss: ' + str(training_loss/s))\n",
        "\n",
        "print(f'Training took {(time.time() - start_time)/60} minutes')\n",
        "#so on our training set, our model predicts the whether the user likes or dislikes a movie 78% of the time"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 1 loss: tensor(0.2411)\n",
            "epoch: 2 loss: tensor(0.2287)\n",
            "epoch: 3 loss: tensor(0.2282)\n",
            "epoch: 4 loss: tensor(0.2283)\n",
            "epoch: 5 loss: tensor(0.2285)\n",
            "epoch: 6 loss: tensor(0.2283)\n",
            "epoch: 7 loss: tensor(0.2281)\n",
            "epoch: 8 loss: tensor(0.2280)\n",
            "epoch: 9 loss: tensor(0.2283)\n",
            "epoch: 10 loss: tensor(0.2281)\n",
            "epoch: 11 loss: tensor(0.2276)\n",
            "epoch: 12 loss: tensor(0.2285)\n",
            "epoch: 13 loss: tensor(0.2278)\n",
            "epoch: 14 loss: tensor(0.2284)\n",
            "epoch: 15 loss: tensor(0.2281)\n",
            "epoch: 16 loss: tensor(0.2283)\n",
            "epoch: 17 loss: tensor(0.2280)\n",
            "epoch: 18 loss: tensor(0.2281)\n",
            "epoch: 19 loss: tensor(0.2280)\n",
            "epoch: 20 loss: tensor(0.2284)\n",
            "Training took 7.624994619687398 minutes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HZMNpe1I5F_"
      },
      "source": [
        "# Part 4 - Testing the RBM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8HbSVs-z1fz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61208703-832f-47e1-badc-d994b343831c"
      },
      "source": [
        "test_loss = 0 #initializing the test loss\n",
        "s = 0. #s is the counter we will use to normalize the test loss by dividing test loss by s\n",
        "\n",
        "#So for the for loop below, our range is num_users because we're just going to be making a prediction for every user during our test\n",
        "for id_user in range(num_users):\n",
        "  v = train_set[id_user:id_user+1] #so here v keeps the training set, because the training set is the input that will be used to activate the hidden neurons for our test set predictions. Because the training set does not have any of the ratings from the test set, we have to use the training set as the input so it can try to predict the ratings of answers it does not contain\n",
        "  vt = test_set[id_user:id_user+1] #vt is the target visible nodes for the final predictions\n",
        "  #ph0 was removed because we only needed it for training, not predicting the test set\n",
        "  \n",
        "  #so below, we only need k as 1 step (1 round trip of gibbs sampling, so visibile -> hidden -> visibile) because we already trained it to get to the answer in 15 steps before. So now that it's trained already, it should be able to predict in a single step\n",
        "  #if statement is just checking to make sure that the target visible nodes has movies with ratings, so it's not just empty\n",
        "  if len(vt[vt >= 0]) > 0:\n",
        "    #So here, we want to sample the visible node (predict the output). To do this, we take the bernoulli sampling element from the sample_h function and use it to get the samples of hidden nodes, which will lead us to get the next visible node values, v\n",
        "    _,h = rbm.sample_h(v) \n",
        "    _,v = rbm.sample_v(h) #so here we are updating our v based on the h we found above, basically getting bernoulli sample of Pv_given_H\n",
        "    test_loss += torch.mean(torch.abs(vt[vt >= 0] - v[vt >= 0])) #the loss is just the mean difference between the original visible node ratings and the final visible node ratings after computing against the hidden nodes\n",
        "    s += 1. #this is just incrementing the counter\n",
        "print('test loss = ' + str(test_loss/s))\n",
        "\n",
        "#Similarly, our model was able to predict whether the use liked or disliked new movies 80% of the time"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test loss = tensor(0.2043)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}